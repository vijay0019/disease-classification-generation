{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "43b84210",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers import AutoTokenizer\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "MAX_LEN=512\n",
    "\n",
    "def parse_ann_file(ann_path):\n",
    "    \"\"\"\n",
    "    Parse annotation files.\n",
    "    \"\"\"\n",
    "    entities = []\n",
    "    with open(ann_path, 'r') as file:\n",
    "        for line in file:\n",
    "            parts = line.strip().split('\\t')\n",
    "            if parts[0].startswith(\"T\"):\n",
    "                label_and_span, text = parts[1], parts[2]\n",
    "                label, span = label_and_span.split(\" \", 1)\n",
    "                if label in {\"SIGN\", \"SYMPTOM\"}:\n",
    "                    ranges = span.split(\";\")\n",
    "                    spans = [(int(start), int(end)) for start, end in (r.split() for r in ranges)]\n",
    "                    entities.append((label, spans, text))\n",
    "    return entities\n",
    "\n",
    "def align_tokens_and_labels(text, entities, tokenizer):\n",
    "    \"\"\"\n",
    "    Align tokens with BIO labels.\n",
    "    \"\"\"\n",
    "    text = text.strip()\n",
    "    tokenized = tokenizer(text, return_offsets_mapping=True, add_special_tokens=False)\n",
    "    tokens = tokenizer.tokenize(text, add_special_tokens=False)\n",
    "    token_spans = tokenized[\"offset_mapping\"]\n",
    "    labels = [\"O\"] * len(tokens)\n",
    "\n",
    "    for label, spans, entity_text in entities:\n",
    "        for start, end in spans:\n",
    "            for idx, (tok_start, tok_end) in enumerate(token_spans):\n",
    "                if tok_start >= start and tok_end <= end:\n",
    "                    if tok_start == start:\n",
    "                        labels[idx] = \"B-HPO\"\n",
    "                    else:\n",
    "                        labels[idx] = \"I-HPO\"\n",
    "\n",
    "\n",
    "    aligned_labels = []\n",
    "    for token, label in zip(tokens, labels):\n",
    "        subwords = tokenizer.tokenize(token)\n",
    "        if len(subwords) == 1:\n",
    "            aligned_labels.append(label)\n",
    "        else:\n",
    "            aligned_labels.append(label)\n",
    "            aligned_labels.extend([\"I-HPO\" if label != \"O\" else \"O\"] * (len(subwords) - 1))\n",
    "\n",
    "    return tokens, aligned_labels\n",
    "\n",
    "def preprocess_data(folder_path, tokenizer):\n",
    "    data = []\n",
    "    for file in os.listdir(folder_path):\n",
    "        if file.endswith(\".txt\"):\n",
    "            txt_path = os.path.join(folder_path, file)\n",
    "            ann_path = txt_path.replace(\".txt\", \".ann\")\n",
    "\n",
    "            with open(txt_path, 'r') as f:\n",
    "                text = f.read()\n",
    "\n",
    "            if os.path.exists(ann_path):\n",
    "                entities = parse_ann_file(ann_path)\n",
    "                tokens, labels = align_tokens_and_labels(text, entities, tokenizer)\n",
    "                data.append((tokens, labels))\n",
    "    return data\n",
    "\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "\n",
    "# Preprocess data\n",
    "train_data = preprocess_data(\"datasets/RareDis-v1/train\", tokenizer)\n",
    "dev_data = preprocess_data(\"datasets/RareDis-v1/dev\", tokenizer)\n",
    "test_data = preprocess_data(\"datasets/RareDis-v1/test\", tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4e3b94ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_len=128):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.label_map = {\"O\": 0, \"B-HPO\": 1, \"I-HPO\": 2}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tokens, labels = self.data[idx]\n",
    "        encoded = self.tokenizer(\n",
    "                                tokens,\n",
    "                                is_split_into_words=True,\n",
    "                                padding=\"max_length\",\n",
    "                                truncation=True,\n",
    "                                max_length=self.max_len,\n",
    "                                return_tensors=\"pt\",\n",
    "                                )\n",
    "\n",
    "        label_ids = [self.label_map[label] for label in labels]\n",
    "        label_ids = label_ids[: self.max_len]\n",
    "        label_ids += [0] * (self.max_len - len(label_ids))\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": encoded[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": encoded[\"attention_mask\"].squeeze(0),\n",
    "            \"labels\": torch.tensor(label_ids, dtype=torch.long),\n",
    "        }\n",
    "\n",
    "# Create dataset\n",
    "train_dataset = NERDataset(train_data, tokenizer)\n",
    "dev_dataset = NERDataset(dev_data, tokenizer)\n",
    "test_dataset = NERDataset(test_data, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e843aed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at emilyalsentzer/Bio_ClinicalBERT and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='138' max='138' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [138/138 00:16, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.359500</td>\n",
       "      <td>0.253984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.193700</td>\n",
       "      <td>0.219017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.137400</td>\n",
       "      <td>0.232527</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=138, training_loss=0.23018282392750616, metrics={'train_runtime': 16.7516, 'train_samples_per_second': 130.555, 'train_steps_per_second': 8.238, 'total_flos': 142865293370112.0, 'train_loss': 0.23018282392750616, 'epoch': 3.0})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\", num_labels=len(train_dataset.label_map))\n",
    "for param in model.parameters():\n",
    "    if not param.is_contiguous():\n",
    "        param.data = param.data.contiguous()\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer, padding=True)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./ner_model\",\n",
    "    logging_strategy='epoch',\n",
    "    eval_strategy='epoch',\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=dev_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "123ff381",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.22048313915729523, 'eval_runtime': 0.6824, 'eval_samples_per_second': 304.818, 'eval_steps_per_second': 19.051, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./ner_model/tokenizer_config.json',\n",
       " './ner_model/special_tokens_map.json',\n",
       " './ner_model/vocab.txt',\n",
       " './ner_model/added_tokens.json',\n",
       " './ner_model/tokenizer.json')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = trainer.evaluate(test_dataset)\n",
    "print(metrics)\n",
    "\n",
    "trainer.save_model(\"./ner_model\")\n",
    "tokenizer.save_pretrained(\"./ner_model\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:.conda-torchenv]",
   "language": "python",
   "name": "conda-env-.conda-torchenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
