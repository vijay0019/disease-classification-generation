{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28d9ffd6",
   "metadata": {},
   "source": [
    "# Bio_ClinicalBERT Finetuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b052782",
   "metadata": {},
   "source": [
    "### Preprocessing the data\n",
    "    - Read text and annotation files\n",
    "    - Tokenize text\n",
    "    - Align with annotations\n",
    "    - Generate BIO tags (labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43b84210",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers import AutoTokenizer\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "def parse_ann_file(ann_path):\n",
    "    \"\"\"\n",
    "    Parse annotation files.\n",
    "    \"\"\"\n",
    "    entities = []\n",
    "    with open(ann_path, 'r') as file:\n",
    "        for line in file:\n",
    "            parts = line.strip().split('\\t')\n",
    "            if parts[0].startswith(\"T\"):\n",
    "                label_and_span, text = parts[1], parts[2]\n",
    "                label, span = label_and_span.split(\" \", 1)\n",
    "                if label in {\"SIGN\", \"SYMPTOM\"}:\n",
    "                    ranges = span.split(\";\")\n",
    "                    spans = [(int(start), int(end)) for start, end in (r.split() for r in ranges)]\n",
    "                    entities.append((label, spans, text))\n",
    "    return entities\n",
    "\n",
    "def align_tokens_and_labels(text, entities, tokenizer):\n",
    "    \"\"\"\n",
    "    Align tokens with BIO labels.\n",
    "    \"\"\"\n",
    "    text = text.strip()\n",
    "    tokenized = tokenizer(text, return_offsets_mapping=True, add_special_tokens=False)\n",
    "    tokens = tokenizer.tokenize(text, add_special_tokens=False)\n",
    "    token_spans = tokenized[\"offset_mapping\"]\n",
    "    labels = [\"O\"] * len(tokens)\n",
    "\n",
    "    for label, spans, entity_text in entities:\n",
    "        for start, end in spans:\n",
    "            for idx, (tok_start, tok_end) in enumerate(token_spans):\n",
    "                if tok_start >= start and tok_end <= end:\n",
    "                    if tok_start == start:\n",
    "                        labels[idx] = \"B-HPO\"\n",
    "                    else:\n",
    "                        labels[idx] = \"I-HPO\"\n",
    "\n",
    "\n",
    "    aligned_labels = []\n",
    "    for token, label in zip(tokens, labels):\n",
    "        subwords = tokenizer.tokenize(token)\n",
    "        if len(subwords) == 1:\n",
    "            aligned_labels.append(label)\n",
    "        else:\n",
    "            aligned_labels.append(label)\n",
    "            aligned_labels.extend([\"I-HPO\" if label != \"O\" else \"O\"] * (len(subwords) - 1))\n",
    "\n",
    "    return tokens, aligned_labels\n",
    "\n",
    "def preprocess_data(folder_path, tokenizer):\n",
    "    data = []\n",
    "    for file in os.listdir(folder_path):\n",
    "        if file.endswith(\".txt\"):\n",
    "            txt_path = os.path.join(folder_path, file)\n",
    "            ann_path = txt_path.replace(\".txt\", \".ann\")\n",
    "\n",
    "            with open(txt_path, 'r') as f:\n",
    "                text = f.read()\n",
    "\n",
    "            if os.path.exists(ann_path):\n",
    "                entities = parse_ann_file(ann_path)\n",
    "                tokens, labels = align_tokens_and_labels(text, entities, tokenizer)\n",
    "                data.append((tokens, labels))\n",
    "    return data\n",
    "\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "\n",
    "# Preprocess data\n",
    "train_data = preprocess_data(\"datasets/RareDis-v1/train\", tokenizer)\n",
    "dev_data = preprocess_data(\"datasets/RareDis-v1/dev\", tokenizer)\n",
    "test_data = preprocess_data(\"datasets/RareDis-v1/test\", tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3cf9574",
   "metadata": {},
   "source": [
    "### Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e3b94ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "MAX_LEN = 512\n",
    "\n",
    "class HPODataset(Dataset):\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.label_map = {\"O\": 0, \"B-HPO\": 1, \"I-HPO\": 2}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tokens, labels = self.data[idx]\n",
    "        encoded = self.tokenizer(tokens,\n",
    "                                is_split_into_words=True,\n",
    "                                padding=\"max_length\",\n",
    "                                truncation=True,\n",
    "                                max_length=MAX_LEN,\n",
    "                                return_tensors=\"pt\",\n",
    "                                )\n",
    "\n",
    "        label_ids = [self.label_map[label] for label in labels]\n",
    "        label_ids = label_ids[:MAX_LEN]\n",
    "        label_ids += [0] * (MAX_LEN - len(label_ids))\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": encoded[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": encoded[\"attention_mask\"].squeeze(0),\n",
    "            \"labels\": torch.tensor(label_ids, dtype=torch.long),\n",
    "        }\n",
    "\n",
    "\n",
    "train_dataset = HPODataset(train_data, tokenizer)\n",
    "dev_dataset = HPODataset(dev_data, tokenizer)\n",
    "test_dataset = HPODataset(test_data, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed93ddb",
   "metadata": {},
   "source": [
    "### Finetuning Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e843aed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at emilyalsentzer/Bio_ClinicalBERT and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='138' max='138' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [138/138 00:50, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.283500</td>\n",
       "      <td>0.171244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.148500</td>\n",
       "      <td>0.138220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.104100</td>\n",
       "      <td>0.139376</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=138, training_loss=0.17871321802553924, metrics={'train_runtime': 50.4691, 'train_samples_per_second': 43.333, 'train_steps_per_second': 2.734, 'total_flos': 571461173480448.0, 'train_loss': 0.17871321802553924, 'epoch': 3.0})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\", num_labels=len(train_dataset.label_map))\n",
    "for param in model.parameters():\n",
    "    if not param.is_contiguous():\n",
    "        param.data = param.data.contiguous()\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer, padding=True)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./ner_model\",\n",
    "    logging_strategy='epoch',\n",
    "    eval_strategy='epoch',\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=dev_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "123ff381",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.14030082523822784, 'eval_runtime': 1.689, 'eval_samples_per_second': 123.146, 'eval_steps_per_second': 7.697, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./saved_hpo_bert/tokenizer_config.json',\n",
       " './saved_hpo_bert/special_tokens_map.json',\n",
       " './saved_hpo_bert/vocab.txt',\n",
       " './saved_hpo_bert/added_tokens.json',\n",
       " './saved_hpo_bert/tokenizer.json')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate and save\n",
    "metrics = trainer.evaluate(test_dataset)\n",
    "print(metrics)\n",
    "\n",
    "trainer.save_model(\"./saved_hpo_bert\")\n",
    "tokenizer.save_pretrained(\"./saved_hpo_bert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f213edca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from torch.nn.functional import softmax\n",
    "import torch\n",
    "\n",
    "model_path = './saved_hpo_bert'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_path)\n",
    "model.eval()\n",
    "labels = {0: \"O\", 1: \"B-HPO\", 2:\"I-HPO\"}\n",
    "\n",
    "def get_hpo_terms(text):\n",
    "    \"\"\"\n",
    "    Extract HPO terms from text.\n",
    "    \n",
    "    Arg:\n",
    "    - text (str): Input text for NER.\n",
    "    \n",
    "    Returns:\n",
    "    - List of recognized HPO terms.\n",
    "    \"\"\"\n",
    "\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    attention_mask = inputs[\"attention_mask\"]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "    \n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "    pred_labels = predictions[0].tolist()\n",
    "    \n",
    "    recognized_entities = []\n",
    "    current_entity = []\n",
    "    \n",
    "    for token, label_id in zip(tokens, pred_labels):\n",
    "        label = labels.get(label_id, \"O\")\n",
    "\n",
    "        if label == \"B-HPO\":\n",
    "            if current_entity:\n",
    "                recognized_entities.append(\" \".join(current_entity))\n",
    "            current_entity = [token]\n",
    "        elif label == \"I-HPO\":\n",
    "            if current_entity:\n",
    "                current_entity.append(token)\n",
    "        else:\n",
    "            if current_entity:\n",
    "                recognized_entities.append(\" \".join(current_entity))\n",
    "                current_entity = []\n",
    "\n",
    "    if current_entity:\n",
    "        recognized_entities.append(\" \".join(current_entity))\n",
    "    \n",
    "    hpo_terms = [\" \".join(e.replace(\" ##\", \"\").replace(\"##\", \"\") for e in entity.split()) for entity in recognized_entities]\n",
    "    \n",
    "    return hpo_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fa3e5677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['include red it chy skin p', 'and chest pain muscular pain my', 'and', 'become abnormal']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "text = \"\"\"Acanthocheilonemiasis is a rare tropical infectious disease caused by a parasite known as Acanthocheilonema perstans, which belongs to a group of parasitic diseases known as filarial diseases (nematode). This parasite is found, for the most part, in Africa. Symptoms of infection may include red, itchy skin (pruritis), abdominal and chest pain, muscular pain (myalgia), and areas of localized swelling (edema). In addition, the liver and spleen may become abnormally enlarged (hepatosplenomegaly). Laboratory testing may also reveal abnormally elevated levels of certain specialized white blood cells (eosinophilia). The parasite is transmitted through the bite of small flies (A. coliroides). Acanthocheilonemiasis is a rare infectious disease caused by long “thread-like” worms, Acanthocheilonema perstans, also known as Dipetalonema perstans. The disease is transmitted by a small black insect (midge), called A. Cailicoides. Acanthocheilonema perstans, the parasite that causes Acanthocheilonemiasis is common in central Africa and in some areas of South America.  This disorder affects males and females in equal numbers. Acanthocheilonemiasis is treated by means of the administration of antifilarial drugs, some of which are newer than others. Ivermectin or diethyl-carbamazine (DEC) are frequently prescribed. Occasionally, surgery may be required to remove large adult worms. Mild cases of acanthocheilonemiasis do not require treatment.\n",
    "\"\"\"\n",
    "text = re.sub(r'[^a-zA-Z\\s]', '', text.strip())\n",
    "print(get_hpo_terms(text))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:.conda-torchenv]",
   "language": "python",
   "name": "conda-env-.conda-torchenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
